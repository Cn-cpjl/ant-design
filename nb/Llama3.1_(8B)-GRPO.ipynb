{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3wUmkh-f5t"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0eMqZVT-f5v"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB92Vzbd-f5v"
      },
      "source": [
        "**Read our [blog post](https://unsloth.ai/blog/r1-reasoning) for guidance on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFZlRlWd-f5w"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rD989Ipi-f5w"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Skip restarting message in Colab\n",
        "import sys; modules = list(sys.modules.keys())\n",
        "for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "\n",
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKuQf3fY-f5x"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyu9Ug2XEt"
      },
      "source": [
        "Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "59DIs5BMcvjN"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8-SLRUB2gwM"
      },
      "source": [
        "Load up `Llama 3.1 8B Instruct`, and set parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "4fe2b0e8-5b1e-44a9-bf42-8b471a549a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
            "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
            "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 2.79%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 128.\n",
            "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 0 GB.\n",
            "WARNING 02-18 08:40:29 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-18 08:40:29 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 02-18 08:40:29 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n",
            "INFO 02-18 08:40:32 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n",
            "Unsloth: Retrying vLLM to process 96 sequences and 256 tokens in tandem.\n",
            "Error:\n",
            "CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 722.12 MiB is free. Process 20422 has 14.03 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, with 146.00 MiB allocated in private pools (e.g., CUDA Graphs), and 152.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "WARNING 02-18 08:40:36 config.py:2386] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 02-18 08:40:36 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 02-18 08:40:36 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":96}, use_cached_outputs=False, \n",
            "INFO 02-18 08:40:38 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 722.12 MiB is free. Process 20422 has 14.03 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, with 146.00 MiB allocated in private pools (e.g., CUDA Graphs), and 152.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/vllm_utils.py\u001b[0m in \u001b[0;36mload_vllm\u001b[0;34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m                 \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mengine_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_engine_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         self.llm_engine = self.engine_class.from_engine_args(\n\u001b[0m\u001b[1;32m    243\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_executor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sleeping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"init_device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollective_rpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"load_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_initialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_current_vllm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpadded_vocab_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_extra_vocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             self.lm_head = ParallelLMHead(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpadded_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, bias, params_dtype, org_num_embeddings, padding_size, quant_config, prefix)\u001b[0m\n\u001b[1;32m    457\u001b[0m                  prefix: str = \"\"):\n\u001b[0;32m--> 458\u001b[0;31m         super().__init__(num_embeddings, embedding_dim, params_dtype,\n\u001b[0m\u001b[1;32m    459\u001b[0m                          \u001b[0morg_num_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, params_dtype, org_num_embeddings, padding_size, quant_config, prefix)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         self.linear_method.create_weights(self,\n\u001b[0m\u001b[1;32m    264\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m\"\"\"Create weights for embedding layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         weight = Parameter(torch.empty(sum(output_partition_sizes),\n\u001b[0m\u001b[1;32m     32\u001b[0m                                        \u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 722.12 MiB is free. Process 20422 has 14.03 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, with 146.00 MiB allocated in private pools (e.g., CUDA Graphs), and 152.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ae5c37a9619f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlora_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;31m# Larger rank = smarter, but slower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/meta-Llama-3.1-8B-Instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m             \u001b[0;31m# Load vLLM first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mload_vllm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[0;31m# Convert to HF format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/vllm_utils.py\u001b[0m in \u001b[0;36mload_vllm\u001b[0;34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"gpu_memory_utilization\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"memory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1008.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 722.12 MiB is free. Process 20422 has 14.03 GiB memory in use. Of the allocated memory 13.32 GiB is allocated by PyTorch, with 146.00 MiB allocated in private pools (e.g., CUDA Graphs), and 152.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from unsloth import is_bfloat16_supported\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ], # Remove QKVO if out of memory\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cXk993X6C2ZZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load and prep dataset\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<think>\n",
        "...\n",
        "</think>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "def load_sql_qa_dataset() -> Dataset:\n",
        "    with open('sql_qa_dataset.json', 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    dataset = [{\n",
        "        'prompt': [\n",
        "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "            {'role': 'user', 'content': example['question']}\n",
        "        ],\n",
        "        'answer': extract_xml_answer(example['answer'])\n",
        "    } for example in data['examples']]\n",
        "\n",
        "    return Dataset.from_list(dataset)\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "dataset = load_sql_qa_dataset()\n",
        "\n",
        "# Reward functions\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    responses = [completion[0]['content'] for completion in completions]\n",
        "    q = prompts[0][-1]['content']\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    # 计算文本相似度\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    rewards = []\n",
        "    for r, a in zip(extracted_responses, answer):\n",
        "        # 处理空字符串情况\n",
        "        if not r.strip() or not a.strip():\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        # 计算TF-IDF向量\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([r, a])\n",
        "            # 计算余弦相似度\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "            # 将相似度映射到[0, 2.0]范围\n",
        "            reward = similarity * 2.0\n",
        "            rewards.append(reward)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating similarity: {e}\")\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"<think>.*?</think>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<think>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</think>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    use_vllm = True, # use vLLM for fast inference!\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    per_device_train_batch_size = 6,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 6, # Decrease if out of memory\n",
        "    max_prompt_length = 256,\n",
        "    max_completion_length = 800,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 250,\n",
        "    save_steps = 250,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "7eaaa07f-4db4-4b7d-990a-e29d3f39a361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 2 | Num Epochs = 125\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient Accumulation steps = 1\n",
            "\\        /    Total batch size = 6 | Total steps = 250\n",
            " \"-____-\"     Number of trainable parameters = 83,886,080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "think\n",
            "SELECT \n",
            "  c.city,\n",
            "  COUNT(DISTINCT o.order_id) AS 活跃客户数,\n",
            "  AVG(o.total_amount) AS 客单价\n",
            "FROM \n",
            "  orders o\n",
            "  JOIN customers c ON o.customer_id = c.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  活跃客户数 DESC\n",
            "</think>\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "  c.city,\n",
            "  COUNT(DISTINCT o.order_id) AS 活跃客户数,\n",
            "  AVG(o.total_amount) AS 客单价\n",
            "FROM \n",
            "  orders o\n",
            "  JOIN customers c ON o.customer_id = c.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  c.city,\n",
            "  COUNT(DISTINCT o.order_id) AS 活跃客户数,\n",
            "  AVG(o.total_amount) AS 客单价\n",
            "FROM \n",
            "  orders o\n",
            "  JOIN customers c ON o.customer_id = c.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='17' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 17/250 13:27 < 3:29:08, 0.02 it/s, Epoch 8/125]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / xmlcount_reward_func</th>\n",
              "      <th>rewards / soft_format_reward_func</th>\n",
              "      <th>rewards / strict_format_reward_func</th>\n",
              "      <th>rewards / correctness_reward_func</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.057667</td>\n",
              "      <td>0.163837</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.057667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>0.118348</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.031167</td>\n",
              "      <td>0.128083</td>\n",
              "      <td>196.333344</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.031167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.104833</td>\n",
              "      <td>0.197872</td>\n",
              "      <td>189.500000</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>-0.104833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.080833</td>\n",
              "      <td>0.091894</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>-0.080833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.059667</td>\n",
              "      <td>0.092987</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>-0.059667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.079500</td>\n",
              "      <td>0.150853</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.079500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031500</td>\n",
              "      <td>0.146725</td>\n",
              "      <td>184.666672</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.031500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.090167</td>\n",
              "      <td>0.131535</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>-0.090167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.086833</td>\n",
              "      <td>0.062933</td>\n",
              "      <td>197.500000</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.086833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.155000</td>\n",
              "      <td>0.165176</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>-0.155000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.058500</td>\n",
              "      <td>0.157703</td>\n",
              "      <td>198.833344</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>-0.058500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.057500</td>\n",
              "      <td>0.091143</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>-0.057500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.021833</td>\n",
              "      <td>0.106289</td>\n",
              "      <td>197.500000</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>-0.021833</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "为了解决这个问题，我们需要从 `orders` 表和 `customers` 表中获取以下信息：\n",
            "\n",
            "1. 活跃客户（客户在指定时间段内至少有一笔订单）\n",
            "2. 平均客单价（客户在指定时间段内的平均订单金额）\n",
            "\n",
            "我们可以使用下面的 SQL 查询来实现这些需求：\n",
            "\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(DISTINCT c.customer_id) AS active_customers,\n",
            "  AVG(o.total_amount) AS average_order_price\n",
            "FROM \n",
            "  customers c\n",
            "JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date < '2023-04-01'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  active_customers DESC;\n",
            "```\n",
            "\n",
            "在这个查询中，我们首先从 `orders` 表和 `customers` 表中关联，选择`city`和`customer \n",
            "Extracted:\n",
            "<think>\n",
            "为了解决这个问题，我们需要从 `orders` 表和 `customers` 表中获取以下信息：\n",
            "\n",
            "1. 活跃客户（客户在指定时间段内至少有一笔订单）\n",
            "2. 平均客单价（客户在指定时间段内的平均订单金额）\n",
            "\n",
            "我们可以使用下面的 SQL 查询来实现这些需求：\n",
            "\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(DISTINCT c.customer_id) AS active_customers,\n",
            "  AVG(o.total_amount) AS average_order_price\n",
            "FROM \n",
            "  customers c\n",
            "JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date < '2023-04-01'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  active_customers DESC;\n",
            "```\n",
            "\n",
            "在这个查询中，我们首先从 `orders` 表和 `customers` 表中关联，选择`city`和`customer\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "我们需要根据给定的表结构和指标定义编写一个SQL语句来实现该需求。我们需要使用子查询来计算活跃客户数和平均客单价。\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(DISTINCT o.customer_id) AS 活跃客户数,\n",
            "  AVG(o.total_amount) AS 平均客单价\n",
            "FROM \n",
            "  customers c\n",
            "  LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            "  WHERE \n",
            "    EXTRACT(YEAR FROM o.order_date) = 2023 AND\n",
            "    EXTRACT(QUARTER FROM o.order_date) = 1\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  活跃客户数 DESC;\n",
            "```\n",
            "该语句使用了左连接来包含所有客户，即使他们没有在 2023 年第一季度有任何订单。\n",
            "\n",
            "*   `EXTRACT(YEAR FROM o.order \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(DISTINCT o.customer_id) AS 活跃客户数,\n",
            "  AVG(o.total_amount) AS 平均客单价\n",
            "FROM \n",
            "  customers c\n",
            "  LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            "  WHERE \n",
            "    EXTRACT(YEAR FROM o.order_date) = 2023 AND\n",
            "    EXTRACT(QUARTER FROM o.order_date) = 1\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  活跃客户数 DESC;\n",
            "```\n",
            "该语句使用了左连接来包含所有客户，即使他们没有在 2023 年第一季度有任何订单。\n",
            "\n",
            "*   `EXTRACT(YEAR FROM o.order\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "<think>\n",
            "在SQL中，可以使用JOIN连接两个表，计算销售额和利润率，并使用WHERE和GROUP BY过滤条件，然后使用HAVING和ORDER BY来实现所需的结果。\n",
            "</think>\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS total_sales,\n",
            "  (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100 AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "解释：\n",
            "1. 首先，我们连接`sales`和`products`表，以便我们可以 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS total_sales,\n",
            "  (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100 AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "解释：\n",
            "1. 首先，我们连接`sales`和`products`表，以便我们可以\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "<think>\n",
            "我们需要计算销售额和利润率，然后使用过滤条件和排序。我们可以使用聚合函数和WHERE子句来实现。\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "    p.category, \n",
            "    SUM(s.quantity * s.unit_price) AS total_sales,\n",
            "    (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) * 100 / SUM(s.quantity * s.unit_price) AS profit_rate\n",
            "FROM \n",
            "    sales s\n",
            "JOIN \n",
            "    products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "    EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "    p.category\n",
            "HAVING \n",
            "    SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "    profit_rate DESC;\n",
            "```\n",
            "此 SQL 查询首先将销售数据与产品数据连接起来，然后计算 2023 年销售额和利润率。最后，使用 HAVING \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "    p.category, \n",
            "    SUM(s.quantity * s.unit_price) AS total_sales,\n",
            "    (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) * 100 / SUM(s.quantity * s.unit_price) AS profit_rate\n",
            "FROM \n",
            "    sales s\n",
            "JOIN \n",
            "    products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "    EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "    p.category\n",
            "HAVING \n",
            "    SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "    profit_rate DESC;\n",
            "```\n",
            "此 SQL 查询首先将销售数据与产品数据连接起来，然后计算 2023 年销售额和利润率。最后，使用 HAVING\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "\n",
            "首先，我们需要确定2023年第一季度的时间段，即2023-01-01至2023-03-31。\n",
            "\n",
            "接下来，我们需要计算每个城市的活跃客户数和平均客单价。由于活跃客户是在指定时间段内至少有一笔订单的客户，我们需要过滤出2023年第一季度内的订单。我们还需要计算每个客户的订单总金额，以便计算平均客单价。\n",
            "\n",
            "最终，我们可以使用`GROUP BY`和`ORDER BY` 来分组和排序结果。\n",
            "\n",
            "```sql\n",
            "SELECT c.city, SUM(CASE WHEN o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31' THEN 1 ELSE 0 END) AS active_customer_count,\n",
            "       AVG(o.total_amount) AS avg_order_price\n",
            "FROM customers c LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            " \n",
            "Extracted:\n",
            "<think>\n",
            "\n",
            "首先，我们需要确定2023年第一季度的时间段，即2023-01-01至2023-03-31。\n",
            "\n",
            "接下来，我们需要计算每个城市的活跃客户数和平均客单价。由于活跃客户是在指定时间段内至少有一笔订单的客户，我们需要过滤出2023年第一季度内的订单。我们还需要计算每个客户的订单总金额，以便计算平均客单价。\n",
            "\n",
            "最终，我们可以使用`GROUP BY`和`ORDER BY` 来分组和排序结果。\n",
            "\n",
            "```sql\n",
            "SELECT c.city, SUM(CASE WHEN o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31' THEN 1 ELSE 0 END) AS active_customer_count,\n",
            "       AVG(o.total_amount) AS avg_order_price\n",
            "FROM customers c LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "<think>\n",
            "首先，我们需要将两个表的相关字段JOIN并计算出销售额和利润率。\n",
            "</think>\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "  p.category,\n",
            "  COALESCE(SUM(s.quantity * s.unit_price), 0) AS total_sales,\n",
            "  ROUND(COALESCE(SUM((s.quantity * s.unit_price - s.quantity * p.cost_price) / (s.quantity * s.unit_price) * 100), 0), 2) AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "  JOIN products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  YEAR(s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "这里我们用**COALESCE**函数来防止在计算时遇到NULL值。如果某个类别的总销售额零 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category,\n",
            "  COALESCE(SUM(s.quantity * s.unit_price), 0) AS total_sales,\n",
            "  ROUND(COALESCE(SUM((s.quantity * s.unit_price - s.quantity * p.cost_price) / (s.quantity * s.unit_price) * 100), 0), 2) AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "  JOIN products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  YEAR(s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "这里我们用**COALESCE**函数来防止在计算时遇到NULL值。如果某个类别的总销售额零\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "首先，我们需要获得2023年第一季度各个城市的活跃客户数和平均客单价。我们可以使用下面的SQL语句：\n",
            "</think>\n",
            "```sql\n",
            "SELECT \n",
            "  c.city,\n",
            "  COUNT(DISTINCT o.customer_id) AS active_customers,\n",
            "  AVG(o.total_amount) AS avg_order_price\n",
            "FROM \n",
            "  customers c\n",
            "  JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  YEAR(o.order_date) = 2023 AND QUARTER(o.order_date) = 1\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  active_customers DESC;\n",
            "```\n",
            "这个语句首先连接`customers`和`orders`表，根据订单日期 year 为 2023 且 quarter 为 1（第一季度）的条件进行过滤。然后使用 `COUNT(DISTINCT)` 函数统计每个城市的活跃客户数，并使用 `AV \n",
            "Extracted:\n",
            "<think>\n",
            "首先，我们需要获得2023年第一季度各个城市的活跃客户数和平均客单价。我们可以使用下面的SQL语句：\n",
            "</think>\n",
            "```sql\n",
            "SELECT \n",
            "  c.city,\n",
            "  COUNT(DISTINCT o.customer_id) AS active_customers,\n",
            "  AVG(o.total_amount) AS avg_order_price\n",
            "FROM \n",
            "  customers c\n",
            "  JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  YEAR(o.order_date) = 2023 AND QUARTER(o.order_date) = 1\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  active_customers DESC;\n",
            "```\n",
            "这个语句首先连接`customers`和`orders`表，根据订单日期 year 为 2023 且 quarter 为 1（第一季度）的条件进行过滤。然后使用 `COUNT(DISTINCT)` 函数统计每个城市的活跃客户数，并使用 `AV\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS 总销售额, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100, 2) AS 利润率\n",
            "FROM \n",
            "  sales s \n",
            "  JOIN products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  利润率 DESC;\n",
            "```\n",
            "\n",
            "### 说明\n",
            "\n",
            "1. 我们首先使用 `JOIN` 关键字连接 `sales` 表和 `products` 表，通过 `product_id` 字段连接两张表。\n",
            "2. `EXTRACT(YEAR FROM s.sale_date)` 表示从 `sale_date` 中提取年 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS 总销售额, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100, 2) AS 利润率\n",
            "FROM \n",
            "  sales s \n",
            "  JOIN products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  利润率 DESC;\n",
            "```\n",
            "\n",
            "### 说明\n",
            "\n",
            "1. 我们首先使用 `JOIN` 关键字连接 `sales` 表和 `products` 表，通过 `product_id` 字段连接两张表。\n",
            "2. `EXTRACT(YEAR FROM s.sale_date)` 表示从 `sale_date` 中提取年\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "我们需要按照年份、季度和城市分组，在指定时间段内过滤出活跃客户，统计活跃客户人数和平均单价。\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "    YEAR(order_date) AS year,\n",
            "    QUARTER(order_date) AS quarter,\n",
            "    c.city,\n",
            "    COUNT(DISTINCT u.customer_id) AS active_customers,\n",
            "    ROUND(AVG(o.total_amount), 2) AS average_single_price\n",
            "FROM \n",
            "    customers c\n",
            "JOIN \n",
            "    orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "    EXTRACT(YEAR FROM o.order_date) = 2023\n",
            "    AND EXTRACT(QUARTER FROM o.order_date) IN (1, 2, 3)\n",
            "GROUP BY \n",
            "    YEAR(order_date),\n",
            "    QUARTER(order_date),\n",
            "    c.city\n",
            "ORDER BY \n",
            "    COUNT(DISTINCT u.customer_id) DESC;\n",
            "```\n",
            "注意 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "    YEAR(order_date) AS year,\n",
            "    QUARTER(order_date) AS quarter,\n",
            "    c.city,\n",
            "    COUNT(DISTINCT u.customer_id) AS active_customers,\n",
            "    ROUND(AVG(o.total_amount), 2) AS average_single_price\n",
            "FROM \n",
            "    customers c\n",
            "JOIN \n",
            "    orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "    EXTRACT(YEAR FROM o.order_date) = 2023\n",
            "    AND EXTRACT(QUARTER FROM o.order_date) IN (1, 2, 3)\n",
            "GROUP BY \n",
            "    YEAR(order_date),\n",
            "    QUARTER(order_date),\n",
            "    c.city\n",
            "ORDER BY \n",
            "    COUNT(DISTINCT u.customer_id) DESC;\n",
            "```\n",
            "注意\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "为了解决这个问题，我们需要使用SQL语句联合 `orders` 和 `customers` 表，计数活跃客户，并计算平均客单价。具体来说，我们可以使用如下语句：\n",
            "</think>\n",
            "<answer>\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(o.order_id) AS 活跃客户数, \n",
            "  AVG(o.total_amount) AS 平均客单价\n",
            "FROM \n",
            "  customers c \n",
            "  LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date < '2023-04-01'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  活跃客户数 DESC;\n",
            "```\n",
            "留意这句join `LEFT JOIN`，因为这样可以包含没有订单的客户oneyr aprèsMate buổi 我 disconnected enjoy problème。 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  c.city, \n",
            "  COUNT(o.order_id) AS 活跃客户数, \n",
            "  AVG(o.total_amount) AS 平均客单价\n",
            "FROM \n",
            "  customers c \n",
            "  LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE \n",
            "  o.order_date >= '2023-01-01' AND o.order_date < '2023-04-01'\n",
            "GROUP BY \n",
            "  c.city\n",
            "ORDER BY \n",
            "  活跃客户数 DESC;\n",
            "```\n",
            "留意这句join `LEFT JOIN`，因为这样可以包含没有订单的客户oneyr aprèsMate buổi 我 disconnected enjoy problème。\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS total_sales, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100, 2) AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "\n",
            "这个 SQL 查询会从 `sales` 表和 `products` 表中选择相关数据。它首先使用 `EXTRACT` 函数从 `sale_date` 中提取年份，然后过滤掉不是 2023 年的数据。它使用 `JOIN`скимопоставить每一行的 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS total_sales, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100, 2) AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "\n",
            "这个 SQL 查询会从 `sales` 表和 `products` 表中选择相关数据。它首先使用 `EXTRACT` 函数从 `sale_date` 中提取年份，然后过滤掉不是 2023 年的数据。它使用 `JOIN`скимопоставить每一行的\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "SELECT DISTINCT\n",
            "    c.city,\n",
            "    COUNT(DISTINCT o.order_id) AS active_customers,\n",
            "    AVG(o.total_amount) AS aver_single_price\n",
            "FROM \n",
            "    customers c\n",
            "    JOIN orders o ON c.customer_id = o.customer_id\n",
            "    JOIN (\n",
            "        SELECT order_id, order_date\n",
            "        FROM orders\n",
            "        WHERE order_date >= '2023-01-01' AND order_date <= '2023-03-31'\n",
            "    ) d ON o.order_id = d.order_id\n",
            "GROUP BY c.city\n",
            "ORDER BY COUNT(DISTINCT o.order_id) DESC;\n",
            "</think>\n",
            "\n",
            "<answer>\n",
            "这段 SQL 查询代码首先从 `orders` 表中筛选出 2023 年第一季度的订单。他通过连接 `customers` 和筛选后的 `orders` 表并GROUP BY city，计算每个城市活跃客户数和平均单价。\n",
            "</answer> \n",
            "Extracted:\n",
            "这段 SQL 查询代码首先从 `orders` 表中筛选出 2023 年第一季度的订单。他通过连接 `customers` 和筛选后的 `orders` 表并GROUP BY city，计算每个城市活跃客户数和平均单价。\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category,\n",
            "  SUM(s.quantity * s.unit_price) AS sales_amount,\n",
            "  (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100 AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "INNER JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  YEAR(s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "\n",
            "**解释**\n",
            "\n",
            "1. 从`sales`和`products`表中查询2023年的数据,根据产品ID关联两个表。\n",
            "2. summation `sales_amount` 为每个类别的总销售额。\n",
            "3. 计算 `profit_rate` 为每个类别的利润率。\n",
            "4. 使用 `GROUP BY` 分组 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category,\n",
            "  SUM(s.quantity * s.unit_price) AS sales_amount,\n",
            "  (SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) / SUM(s.quantity * s.unit_price) * 100 AS profit_rate\n",
            "FROM \n",
            "  sales s\n",
            "INNER JOIN \n",
            "  products p ON s.product_id = p.product_id\n",
            "WHERE \n",
            "  YEAR(s.sale_date) = 2023\n",
            "GROUP BY \n",
            "  p.category\n",
            "HAVING \n",
            "  SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY \n",
            "  profit_rate DESC;\n",
            "```\n",
            "\n",
            "**解释**\n",
            "\n",
            "1. 从`sales`和`products`表中查询2023年的数据,根据产品ID关联两个表。\n",
            "2. summation `sales_amount` 为每个类别的总销售额。\n",
            "3. 计算 `profit_rate` 为每个类别的利润率。\n",
            "4. 使用 `GROUP BY` 分组\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_id INT,\n",
            "  order_date DATE,\n",
            "  total_amount DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "  item_id INT PRIMARY KEY,\n",
            "  order_id INT,\n",
            "  product_id INT,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE customers (\n",
            "  customer_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  city VARCHAR(50),\n",
            "  join_date DATE\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "orders.order_date, orders.total_amount, customers.customer_name, customers.city\n",
            "\n",
            "指标定义：\n",
            "- 活跃客户：在指定时间段内至少有一笔订单的客户\n",
            "- 客单价：客户在指定时间段内的平均订单金额\n",
            "\n",
            "查询要求：\n",
            "查询2023年第一季度各个城市的活跃客户数和平均客单价，按照活跃客户数降序排列。 \n",
            "Answer:\n",
            "SELECT\n",
            "  c.city,\n",
            "  COUNT(DISTINCT c.customer_id) as active_customers,\n",
            "  AVG(o.total_amount) as avg_customer_amount\n",
            "FROM customers c\n",
            "INNER JOIN orders o ON c.customer_id = o.customer_id\n",
            "WHERE o.order_date BETWEEN '2023-01-01' AND '2023-03-31'\n",
            "GROUP BY c.city\n",
            "ORDER BY active_customers DESC; \n",
            "Response:\n",
            "<think>\n",
            "我们需要使用SQL语句来实现该功能，首先我们需要找出2023年第一季度的订单，并把结果简化为来交叉变换成活动客户。\n",
            "\n",
            "```sql\n",
            "SELECT \n",
            "    c.customer_name,\n",
            "    c.city,\n",
            "    COUNT(o.order_id) AS active_customers,\n",
            "    AVG(o.total_amount) AS avg_order_price\n",
            "FROM \n",
            "    orders o\n",
            "    JOIN customers c ON o.customer_id = c.customer_id\n",
            "WHERE \n",
            "    o.order_date >= '2023-01-01' AND o.order_date <= '2023-03-31'\n",
            "GROUP BY \n",
            "    c.customer_name, c.city\n",
            "ORDER BY \n",
            "    active_customers DESC;\n",
            "```\n",
            "\n",
            "但是上述语句无法根据客户的城市进行分组，由于有多行记录，有可能将需要使用子查询或者CTE来实现。\n",
            "</think>\n",
            "<answer>\n",
            "```sql\n",
            "WITH \n",
            "    actives AS (\n",
            "    SELECT \n",
            " \n",
            "Extracted:\n",
            "```sql\n",
            "WITH \n",
            "    actives AS (\n",
            "    SELECT\n",
            "-------------------- Question:\n",
            "表结构：\n",
            "CREATE TABLE sales (\n",
            "  sale_id INT PRIMARY KEY,\n",
            "  product_id INT,\n",
            "  sale_date DATE,\n",
            "  quantity INT,\n",
            "  unit_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "  product_id INT PRIMARY KEY,\n",
            "  product_name VARCHAR(100),\n",
            "  category VARCHAR(50),\n",
            "  cost_price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "参考字段：\n",
            "sales.sale_date, sales.quantity, sales.unit_price, products.category, products.cost_price\n",
            "\n",
            "指标定义：\n",
            "- 销售额：quantity * unit_price\n",
            "- 利润率：(销售额 - quantity * cost_price) / 销售额 * 100%\n",
            "\n",
            "查询要求：\n",
            "查询2023年各产品类别的总销售额和利润率，只显示总销售额超过10000的类别，按利润率降序排序。 \n",
            "Answer:\n",
            "WITH sales_data AS (\n",
            "  SELECT\n",
            "    p.category,\n",
            "    SUM(s.quantity * s.unit_price) as total_sales,\n",
            "    SUM(s.quantity * s.unit_price - s.quantity * p.cost_price) as total_profit\n",
            "  FROM sales s\n",
            "  INNER JOIN products p ON s.product_id = p.product_id\n",
            "  WHERE YEAR(s.sale_date) = 2023\n",
            "  GROUP BY p.category\n",
            "  HAVING total_sales > 10000\n",
            ")\n",
            "SELECT\n",
            "  category,\n",
            "  total_sales,\n",
            "  (total_profit / total_sales * 100) as profit_margin\n",
            "FROM sales_data\n",
            "ORDER BY profit_margin DESC; \n",
            "Response:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS 总销售额, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) * 100 / SUM(s.quantity * s.unit_price), 2) AS 利润率\n",
            "FROM sales s\n",
            "JOIN products p ON s.product_id = p.product_id\n",
            "WHERE EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY p.category\n",
            "HAVING SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY 利润率 DESC;\n",
            "```\n",
            "解释：\n",
            "1. 使用 `EXTRACT(YEAR FROM s.sale_date)` 函数提取销售日期的年份，并限制 2023 年的记录。\n",
            "2. JOIN 连接 `sales` 和 `products` 表，根据 `product_id` 进行匹配。\n",
            "3. 使用 `SUM` 函数计算各类别的总销售额 \n",
            "Extracted:\n",
            "```sql\n",
            "SELECT \n",
            "  p.category, \n",
            "  SUM(s.quantity * s.unit_price) AS 总销售额, \n",
            "  ROUND((SUM(s.quantity * s.unit_price) - SUM(s.quantity * p.cost_price)) * 100 / SUM(s.quantity * s.unit_price), 2) AS 利润率\n",
            "FROM sales s\n",
            "JOIN products p ON s.product_id = p.product_id\n",
            "WHERE EXTRACT(YEAR FROM s.sale_date) = 2023\n",
            "GROUP BY p.category\n",
            "HAVING SUM(s.quantity * s.unit_price) > 10000\n",
            "ORDER BY 利润率 DESC;\n",
            "```\n",
            "解释：\n",
            "1. 使用 `EXTRACT(YEAR FROM s.sale_date)` 函数提取销售日期的年份，并限制 2023 年的记录。\n",
            "2. JOIN 连接 `sales` 和 `products` 表，根据 `product_id` 进行匹配。\n",
            "3. 使用 `SUM` 函数计算各类别的总销售额\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-62c5befec131>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
            "\u001b[0;32m/content/unsloth_compiled_cache/UnslothGRPOTrainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mall_prompts_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_prompts_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grpo_trainer_lora_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m                 \u001b[0mcompletion_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompletions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                     )\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    467\u001b[0m             priority=priority)\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_tqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequestOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m_run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mtotal_out_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_unfinished_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mstep_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstep_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1384\u001b[0m                     virtual_engine]\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             outputs = self.model_executor.execute_model(\n\u001b[0m\u001b[1;32m   1387\u001b[0m                 execute_model_req=execute_model_req)\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecute_model_req\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExecuteModelRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 138\u001b[0;31m         output = self.collective_rpc(\"execute_model\",\n\u001b[0m\u001b[1;32m    139\u001b[0m                                      args=(execute_model_req, ))\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/executor/uniproc_executor.py\u001b[0m in \u001b[0;36mcollective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/utils.py\u001b[0m in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/worker_base.py\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \"model_execute_time\", torch.tensor(0)).item()\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         output = self.model_runner.execute_model(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mmodel_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mkv_caches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_engine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mexecute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;31m# Sample the next token.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m         output: SamplerOutput = self.model.sample(\n\u001b[0m\u001b[1;32m   1776\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0msampling_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/models/llama.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    555\u001b[0m     def sample(self, logits: torch.Tensor,\n\u001b[1;32m    556\u001b[0m                sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/sampler.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# Sample the next tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         maybe_deferred_sample_results, maybe_sampled_tokens_tensor = _sample(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/sampler.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0msampled_token_ids_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0msampled\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \"\"\"\n\u001b[0;32m--> 853\u001b[0;31m     return _sample_with_torch(\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/sampler.py\u001b[0m in \u001b[0;36m_sample_with_torch\u001b[0;34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# This also converts the sampler output to a Python object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;31m# Return Pythonized sampler result & sampled token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         return get_pythonized_sample_results(\n\u001b[0m\u001b[1;32m    823\u001b[0m             maybe_deferred_args), sampled_token_ids_tensor\n\u001b[1;32m    824\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/sampler.py\u001b[0m in \u001b[0;36mget_pythonized_sample_results\u001b[0;34m(sample_result_args)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msample_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_greedy_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msampling_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSamplingType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRANDOM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSamplingType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             sample_results = _random_sample(seq_groups,\n\u001b[0m\u001b[1;32m    688\u001b[0m                                             multinomial_samples[sampling_type])\n\u001b[1;32m    689\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msampling_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSamplingType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEAM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/model_executor/layers/sampler.py\u001b[0m in \u001b[0;36m_random_sample\u001b[0;34m(selected_seq_groups, random_samples)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \"\"\"\n\u001b[1;32m    485\u001b[0m     \u001b[0;31m# Find the maximum n value of the prompt phase requests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mrandom_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSampleResultType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "qtcz_lpbVC92",
        "outputId": "9b12655a-7905-42a8-d6f0-210ff74a6d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.78s/it, est. speed input: 1.64 toks/s, output: 19.94 toks/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Calculating pi to a large number of decimal places is a complex task that requires a computational approach, rather than a simple mathematical formula. Here\\'s a way to calculate pi using the Monte Carlo method, which is an approximation method that uses random numbers to estimate the value of pi:\\n\\n**The Monte Carlo Method**\\n\\nThe Monte Carlo method is based on the idea of simulating the probability of a random walk across a square and circle. Here\\'s the basic idea:\\n\\n1. Draw a square and a circle on a piece of paper.\\n2. Generate random points within the square.\\n3. Count the proportion of points that fall within the circle.\\n4. The ratio of points within the circle to the total number of points is approximately equal to the ratio of the area of the circle to the area of the square, which is pi.\\n\\n**Mathematical Formulation**\\n\\nLet\\'s denote the following variables:\\n\\n*   `N`: the number of random points generated\\n*   `n`: the number of points within the circle\\n*   `pi_approx`: the approximated value of pi\\n\\nThe formula to calculate pi is:\\n\\n`pi_approx = (4 * n) / N`\\n\\n**Python Code**\\n\\nHere\\'s a simple Python code snippet to calculate pi using the Monte Carlo method:\\n\\n```python\\nimport random\\nimport math\\n\\ndef calculate_pi(num_points):\\n    # Generate random points within the square (-1, -1) to (1, 1)\\n    points_inside_circle = 0\\n    for _ in range(num_points):\\n        x, y = random.uniform(-1, 1), random.uniform(-1, 1)\\n        # Check if the point falls within the circle (radius 1)\\n        if x**2 + y**2 <= 1:\\n            points_inside_circle += 1\\n\\n    # Calculate pi using the Monte Carlo method\\n    pi_approx = (4 * points_inside_circle) / num_points\\n    return pi_approx\\n\\nnum_points = 1000000\\npi_approx = calculate_pi(num_points)\\nprint(f\"Approximated pi: {pi_approx}\")\\nprint(f\"Difference between approximated pi and actual pi: {abs(pi_approx - math.pi)}\")\\n```\\n\\n**Note**: The more points you generate, the more accurate the approximation will be.\\n\\n**Limitations**\\n\\nThis method has a few limitations:\\n\\n'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "zf_OY5WMVOxF",
        "outputId": "c34d81a7-192d-427d-81f0-cbca7009b7d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:23<00:00, 23.29s/it, est. speed input: 2.62 toks/s, output: 19.41 toks/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"<reasoning>\\nPi (π) is an irrational number that represents the ratio of a circle's circumference to its diameter. It is approximately equal to 3.14159, but its decimal representation goes on indefinitely without repeating.\\n\\nTo calculate pi, we can use various mathematical formulas and methods, such as the Leibniz formula, the Gregory-Leibniz series, or the Monte Carlo method. However, these methods are not practical for obtaining a high degree of accuracy.\\n\\nA more practical approach is to use the Bailey-Borwein-Plouffe (BBP) formula, which is a spigot algorithm that allows us to calculate any digit of pi without having to compute the preceding digits.\\n\\nAnother method is to use the Chudnovsky algorithm, which is a fast and efficient method for calculating pi to a high degree of accuracy.\\n\\nFor simplicity, we can use the first few terms of the BBP formula to estimate pi:\\nπ = 3 + 1/(4/3 - 1/(4/3 - 1/(4/3 - ...))\\n\\nLet's use this simplified formula to estimate pi:\\n\\nπ ≈ 3 + 1/(4/3) ≈ 3 + 1.3333 ≈ 4.3333\\n\\nNow, let's add the next term:\\nπ ≈ 4.3333 + 1/(4/3 - 1/(4/3)) ≈ 4.3333 + 1/(1.3333 - 0.3333) ≈ 4.3333 + 0.6667 ≈ 5.0000\\n\\nNext term:\\nπ ≈ 5.0000 + 1/(1.3333 - 1/(1.3333 - 1/(1.3333))) ≈ 5.0000 + 1/(0.6667 - 0.3333) ≈ 5.0000 + 0.3333 ≈ 5.3333\\n\\nContinuing this process, we can obtain more accurate approximations of pi. However, for a more accurate answer, we would need to use a computer program or a calculator.\\n\\nA more precise calculation using a computer or calculator would give us a\""
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
        "    {\"role\" : \"user\", \"content\" : \"Calculate pi.\"},\n",
        "], tokenize = False, add_generation_prompt = True)\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 0.8,\n",
        "    top_p = 0.95,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpP69pxA-f55"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}